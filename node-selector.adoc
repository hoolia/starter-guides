[[regions-and-zones]]
## Regions and Zones

If you think you're about to learn how to configure regions and zones in
OpenShift 3, you're only partially correct.

In OpenShift 2, we introduced the specific concepts of "regions" and
"zones" to enable organizations to provide some topologies for
application resiliency. Apps would be spread throughout the zones in a
region and, depending on the way you configured OpenShift, you could
make different regions accessible to users.

The reason that you're only "partially" correct in your assumption is
that, for OpenShift 3, Kubernetes doesn't actually care about your
topology. In other words, OpenShift is "topology agnostic". In fact,
OpenShift 3 provides advanced controls for implementing whatever
topologies you can dream up, leveraging filtering and affinity rules to
ensure that parts of applications (pods) are either grouped together or
spread apart.

For the purposes of a simple example, we'll be sticking with the
"regions" and "zones" theme. But, as you go through these examples,
think about what other complex topologies you could implement. Perhaps
"secure" and "insecure" hosts, or other topologies.

First, we need to talk about the "scheduler" and its default
configuration.

[[scheduler-and-defaults]]
### Scheduler and Defaults

The "scheduler" is essentially the OpenShift master. Any time a pod
needs to be created (instantiated) somewhere, the master needs to figure
out where to do this. This is called "scheduling". The default
configuration for the scheduler looks like the following JSON (although
this is embedded in the OpenShift code and you won't find this in a
file):

....
{
  "predicates" : [
    {"name" : "PodFitsResources"},
    {"name" : "MatchNodeSelector"},
    {"name" : "HostName"},
    {"name" : "PodFitsPorts"},
    {"name" : "NoDiskConflict"}
  ],"priorities" : [
    {"name" : "LeastRequestedPriority", "weight" : 1},
    {"name" : "ServiceSpreadingPriority", "weight" : 1}
  ]
}
....

When the scheduler tries to make a decision about pod placement, first
it goes through "predicates", which essentially filter out the possible
nodes we can choose. Note that, depending on your predicate
configuration, you might end up with no possible nodes to choose. This
is totally OK (although generally not desired).

These default options are documented in the link above, but the quick
overview is:

* Place pod on a node that has enough resources for it (duh)
* Place pod on a node that doesn't have a port conflict (duh)
* Place pod on a node that doesn't have a storage conflict (duh)

And some more obscure ones:

* Place pod on a node whose `NodeSelector` matches
* Place pod on a node whose hostname matches the `Host` attribute value

The next thing is, of the available nodes after the filters are applied,
how do we select the "best" one. This is where "priorities" come in.
Long story short, the various priority functions each get a score,
multiplied by the weight, and the node with the highest score is
selected to host the pod.

Again, the defaults are:

* Choose the node that is "least requested" (the least busy)
* Spread services around - minimize the number of pods in the same
service on the same node

And, for an extremely detailed explanation about what these various
configuration flags are doing, check out:

....
https://docs.openshift.com/enterprise/latest/admin_guide/scheduler.html
....

In a small environment, these defaults are pretty sane. Let's look at
one of the important predicates (filters) before we move on to "regions"
and "zones".

[[the-nodeselector]]
### The NodeSelector

`NodeSelector` is a part of the Pod data model. And, if we think back to
our pod definition, there was a "label", which is just a key:value pair.
In the case of a `NodeSelector`, our labels (key:value pairs) are used
to help us try to find nodes that match, assuming that:

* The scheduler is configured to MatchNodeSelector
* The end user creating the pod knows which labels are out there

But this use case is also pretty simplistic. It doesn't really allow for
a topology, and there's not a lot of logic behind it. Also, if I specify
a NodeSelector label when using MatchNodeSelector and there are no
matching nodes, my workload will never get scheduled. Bummer.

How can we make this more intelligent? We'll finally use "regions" and
"zones".

[[examining-the-scheduler-configuration]]
### Examining the Scheduler Configuration

The installer is configured to understand "regions" and "zones" as a
matter of convenience. However, for the master (scheduler) to actually
do something with them requires changing from the default configuration
Take a look at `/etc/origin/master/master-config.yaml` and find the line
with `schedulerConfigFile`.

You should see:

....
schedulerConfigFile: "/etc/origin/master/scheduler.json"
....

Then, take a look at `/etc/origin/master/scheduler.json`. It will have
the following content:

....
{
  "predicates": [
    {"name": "MatchNodeSelector"},
    {"name": "PodFitsResources"},
    {"name": "PodFitsPorts"},
    {"name": "NoDiskConflict"},
    {"name": "Region", "argument": {"serviceAffinity" : {"labels" : ["region"]}}}
  ],"priorities": [
    {"name": "LeastRequestedPriority", "weight": 1},
    {"name": "ServiceSpreadingPriority", "weight": 1},
    {"name": "Zone", "weight" : 2, "argument": {"serviceAntiAffinity" : {"label": "zone"}}}
  ]
}
....

To quickly review the above (this explanation sort of assumes that you
read the scheduler documentation, but it's not critically important):

* Filter out nodes that don't fit the resources, don't have the ports,
or have disk conflicts
* If the pod specifies a label with the key "region", filter nodes by
the value.

So, if we have the following nodes and the following labels:

* Node 1 -- "region":"infra"
* Node 2 -- "region":"primary"
* Node 3 -- "region":"primary"

If we try to schedule a pod that has a `NodeSelector` of
"region":"primary", then only Node 2 and Node 3 would be considered.

OK, that takes care of the "region" part. What about the "zone" part?

Our priorities tell us to:

* Score the least-busy node higher
* Score any nodes who don't already have a pod in this service higher
* Score any nodes whose zone label's value *does not* match higher

Why do we score a zone that *doesn't* match higher? Note that the
definition for the Zone priority is a `serviceAntiAffinity` -- anti
affinity. In this case, our anti affinity rule helps to ensure that we
try to get nodes from _different_ zones to take our pod.

If we consider that our "primary" region might be a certain datacenter,
and that each "zone" in that datacenter might be on its own power system
with its own dedicated networking, this would ensure that, within the
datacenter, pods of an application would be spread across power/network
segments.

The documentation link has some more complicated examples. The
topoligical possibilities are endless!

[[node-labels]]
### Node Labels

The assignments of "regions" and "zones" at the node-level are handled
by labels on the nodes. Since the installation process configured the
regions and zones, but did not ask us how to put the nodes into that
topology, you need to label the nodes now:

....
oc label node/ose3-master.example.com region=infra zone=default
oc label node/ose3-node1.example.com region=primary zone=east
oc label node/ose3-node2.example.com region=primary zone=west
....

You can look at how the labels were implemented by doing:

....
oc get nodes --show-labels |grep region=

NAME                      LABELS                                                                     STATUS                     AGE
ose3-master.example.com   kubernetes.io/hostname=ose3-master.example.com,region=infra,zone=default   Ready,SchedulingDisabled   48m
ose3-node1.example.com    kubernetes.io/hostname=ose3-node1.example.com,region=primary,zone=east     Ready                      48m
ose3-node2.example.com    kubernetes.io/hostname=ose3-node2.example.com,region=primary,zone=west     Ready                      48m
....

At this point we have a running OpenShift environment across three
hosts, with one master and three nodes, divided up into two regions --
"__infra__structure" and "primary". _BUT_ the master is currently tagged
as "SchedulingDisabled". The installer will, by default, not configure
the master's node to receive workload (SchedulingDisabled). You will fix
this in a moment.

[[edit-default-nodeselector]]
### Edit Default NodeSelector

We want our apps to land in the primary region, and not in the infra
region. We can do this by setting a default `nodeSelector` for our
OpenShift environment. Edit the `/etc/origin/master/master-config.yaml`
again, and make the following change:

....
projectConfig:
  defaultNodeSelector: "region=primary"
....

Once complete, restart your master. This will make both our default
`nodeSelector` and routing changes take effect:

....
systemctl restart atomic-openshift-master
....

[[make-master-schedulable]]
### Make Master Schedulable

A single command can be used to make the master node schedulable:

....
oadm manage-node ose3-master.example.com --schedulable=true
....

Then, run the following:

....
oc get node
....

You should see that now your master is set to receive workloads:

....
NAME                      LABELS                                                                     STATUS    AGE
ose3-master.example.com   kubernetes.io/hostname=ose3-master.example.com,region=infra,zone=default   Ready     51m
ose3-node1.example.com    kubernetes.io/hostname=ose3-node1.example.com,region=primary,zone=east     Ready     51m
ose3-node2.example.com    kubernetes.io/hostname=ose3-node2.example.com,region=primary,zone=west     Ready     51m
....

[[tweak-default-project]]
### Tweak Default Project

The _default_ project/namespace is a special one where we will put some
of our infrastructure-related resources. This project was created when
OpenShift was first started (OpenShift always ensures it exists). We
want resources deployed into the _default_ project to run on the
__infra__structure nodes.

Since the _default_ project was created when OpenShift was first
started, it didn't inherit any default `nodeSelector`. Further, we have
configured a default `nodeSelector` for _primary_, not the _infra_
region. So let's make a tweak so that things that go in the _infra_
region.

Execute the following:

....
oc edit namespace explore-XX
....

In the annotations list, add this line:

....
openshift.io/node-selector: region=infra
....

Save and exit the editor. Remember, indentation matters -- this entry
should be at the same indentation level as the rest of the
`openshift.io` items.

From here we will start to deploy "applications" and other resources
into OpenShift.
